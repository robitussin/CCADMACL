{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Classification\n",
        "\n",
        "We will predict the cut quality of diamonds based on their price and other physical measurements. This dataset is built into the Seaborn library."
      ],
      "metadata": {
        "id": "oHYX-xyLESAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "__lj8v_jEHiX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Data"
      ],
      "metadata": {
        "id": "A9LTfd8HEp6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the diamonds dataset from Seaborn\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Split data into features and target\n",
        "X = diamonds.drop(\"cut\", axis=1)\n",
        "y = diamonds[\"cut\"]"
      ],
      "metadata": {
        "id": "f-whi94dEJwC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "ZMC559aKFN0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical features\n",
        "categorical_features = X.select_dtypes(\n",
        "   include=[\"object\"]\n",
        ").columns.tolist()\n",
        "\n",
        "numerical_features = X.select_dtypes(\n",
        "   include=[\"float64\", \"int64\"]\n",
        ").columns.tolist()"
      ],
      "metadata": {
        "id": "AiTntjzjE5pt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "   transformers=[\n",
        "       (\"cat\", OneHotEncoder(), categorical_features),\n",
        "       (\"num\", StandardScaler(), numerical_features),\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "DTIVEadhFI4T"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the hyper parameters\n",
        "\n",
        "`learning_rate`: The most important hyperparameter of gradient boosting is perhaps the learning rate. It controls the contribution of each weak learner by adjusting the shrinkage factor. Smaller values (towards 0) decreases how much say each weak learner has in the ensemble. This requires building more trees and, thus, more time to finish training. But, the final strong learner will indeed be strong and impervious to overfitting.\n",
        "\n",
        "`n_estimators`: This parameter, also called the number of boosting rounds or n_estimators, controls the number of trees to build. The more trees you build, the stronger and more performant the ensemble becomes. It also becomes more complex as more trees allows the model to capture more patterns in the data. However, more trees significantly improve the chances of overfitting. To mitigate this, employ a combination of early stopping and low learning rate.\n",
        "\n",
        "`max_depth`: This parameter controls the number of levels in each weak learner (decision tree). A max depth of 3 means there are three levels in the tree, counting the leaf level. The deeper the tree, the more complex and computationally expensive the model becomes. Choose a value close to 3 to prevent overfitting. Your maximum should be a depth of 10.\n",
        "\n",
        "`min_samples_split`: This parameter controls how branches split in decision trees. Setting a low value for the number of samples in termination nodes (leaves) makes the overall algorithm sensitive to noise. A larger minimum number of samples helps to prevent overfitting by making it more difficult for the trees to create splits based on too few data points.\n",
        "\n",
        "`subsample`: This parameter controls the proportion of the data used to train each tree. real-world datasets often have a lot of rows and require sampling. So, if you set the subsampling rate to a value below 1, such as 0.7, each weak learner trains on the randomly sampled 70% of the rows. A smaller subsample rate can lead to faster training but can also lead to overfitting.\n",
        "\n",
        "`loss`: loss function to optimize. MSE is used in this case however,"
      ],
      "metadata": {
        "id": "ywCN4ZZAH6yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"n_estimators\": 500,\n",
        "    \"max_depth\": 4,\n",
        "    \"min_samples_split\": 5,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"loss\": 'friedman_mse',\n",
        "}"
      ],
      "metadata": {
        "id": "gk9zrHayH7A_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Gradient Boosting Classifier pipeline"
      ],
      "metadata": {
        "id": "NPQsSiBhFWhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(\n",
        "   [\n",
        "       (\"preprocessor\", preprocessor),\n",
        "       (\"classifier\", GradientBoostingClassifier(random_state=42)),\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "dW6a1lrnFEVv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Data"
      ],
      "metadata": {
        "id": "879Fji2WEvSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "-eizARNsEQKu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation and training"
      ],
      "metadata": {
        "id": "7zW3NQRlFo8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "f2-BzKjJFgLY"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report the final results"
      ],
      "metadata": {
        "id": "3Z-UNA_EFmRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liIbTikTFhiL",
        "outputId": "67229300-cd72-44b1-fded-1520d736a361"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cross-Validation Accuracy: 0.7621\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fair       0.90      0.91      0.91       335\n",
            "        Good       0.81      0.63      0.71      1004\n",
            "       Ideal       0.82      0.91      0.86      4292\n",
            "     Premium       0.70      0.86      0.77      2775\n",
            "   Very Good       0.66      0.41      0.51      2382\n",
            "\n",
            "    accuracy                           0.76     10788\n",
            "   macro avg       0.78      0.74      0.75     10788\n",
            "weighted avg       0.75      0.76      0.75     10788\n",
            "\n"
          ]
        }
      ]
    }
  ]
}